{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# library.py\n",
        "import wikipedia as wiki\n",
        "from nltk import word_tokenize, sent_tokenize\n",
        "from nltk.stem import \tWordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "cd_data = 'data/'\n",
        "cd_figures = 'figures/'\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "\n",
        "class StopWords:\n",
        "    def __init__(self, cd_data=''):\n",
        "        \"\"\"\n",
        "        Loads in stops words as a list from the \"stop_words.txt file\" as a list.\n",
        "        \"\"\"\n",
        "        with open(cd_data+'stop_words.txt', 'r+') as file:\n",
        "            self.words = [word.strip('\\n').lower() for word in file.readlines()]\n",
        "\n",
        "    def add_word(self, words):\n",
        "        \"\"\"\n",
        "        Adds a word to the in-memory list.\n",
        "        This does not write to the stop_words.txt file\n",
        "         \"\"\"\n",
        "        self.words.append(words)\n",
        "\n",
        "    def summary(self):\n",
        "        \"\"\"\n",
        "        Prints summary information about stop the stop words list.\n",
        "        - Currently only reporting number of words.\n",
        "        \"\"\"\n",
        "        print('Number of words: ', len(self.words))\n",
        "\n",
        "def load_wiki_article(article_name='Rules of chess', cd_data=''):\n",
        "    \"\"\"\n",
        "    Loads in the wiki article \"Rules of chess\" from Wikipedia and saves it\n",
        "    to a .txt file.\n",
        "    \"\"\"\n",
        "    article = wiki.page(article_name).content\n",
        "    with open(cd_data+article_name+'.txt', 'w+') as file:\n",
        "        file.write(article)\n",
        "\n",
        "def read_wiki_article(article_name='Rules of chess', cd_data=''):\n",
        "    \"\"\"\n",
        "    Reads the wiki article off of the saved .txt file and\n",
        "    returns it as a string.\n",
        "    \"\"\"\n",
        "    with open(cd_data+article_name+'.txt', 'r+') as file:\n",
        "        article = file.read()\n",
        "    return article\n",
        "\n",
        "def tokenize(doc):\n",
        "    \"\"\"\n",
        "    Makes use of NLTK's words tokenizer on a document string and changed all\n",
        "    words to lower case.\n",
        "    - Returns a list of lower case tokenized strings.\n",
        "    \"\"\"\n",
        "    doc_word = word_tokenize(doc.lower())\n",
        "    return doc_word\n",
        "\n",
        "def lemmatize(doc):\n",
        "    \"\"\"\n",
        "    Uses the previous \"tokenize\" function and NLTK's lemmatizer to lemmatize\n",
        "    the document and return it as a list.\n",
        "    \"\"\"\n",
        "    doc_word = word_tokenize(doc)\n",
        "    doc_lemma = []\n",
        "    for word in doc_word:\n",
        "        doc_lemma.append(lemmatizer.lemmatize(word))\n",
        "    return doc_lemma\n",
        "\n",
        "def remove_stopwords(doc):\n",
        "    \"\"\"\n",
        "    Uses the previous \"lemmatize\" function and removes all stop words loaded in\n",
        "    from the \"StopWords\" class. returns a list of strings with the stop words\n",
        "    removed.\n",
        "    \"\"\"\n",
        "    stop = StopWords(cd_data=cd_data)\n",
        "    doc_stop = []\n",
        "    for word in lemmatize(doc):\n",
        "        if word.lower() not in stop.words:\n",
        "            doc_stop.append(word)\n",
        "    return doc_stop\n",
        "\n",
        "def one_hot(doc):\n",
        "    \"\"\"\n",
        "    Uses the previous \"remove_stopwords\" function and one_hot encodes them\n",
        "    using the Pandas.get_dummies method. Returns a Pandas dataframe.\n",
        "    \"\"\"\n",
        "    doc_stop = remove_stopwords(doc)\n",
        "    df_1h = pd.get_dummies(doc_stop)\n",
        "    return df_1h\n",
        "\n",
        "class ProcessedArticle:\n",
        "    def __init__(self, doc):\n",
        "        \"\"\"\n",
        "        Organizes the tokenize, lemmatize, remove_stopwords, and one_hot\n",
        "        functions so that the document only needs to be passed when the class\n",
        "        was instantiated.\n",
        "        \"\"\"\n",
        "        self.doc = doc\n",
        "        self.tokenize = tokenize(self.doc)\n",
        "        self.sent_tokenize = sent_tokenize(self.doc)\n",
        "        self.lemmatize = lemmatize(self.doc)\n",
        "        self.remove_stopwords = remove_stopwords(self.doc)\n",
        "        self.one_hot = one_hot(self.doc)\n",
        "\n",
        "\n",
        "def batch_data(data, num_batches=10):\n",
        "    \"\"\"\n",
        "    Splits an array into batches of a specified size.\n",
        "    \"\"\"\n",
        "    batch_size = round(len(data)/num_batches)\n",
        "    batches = []\n",
        "    size = len(data)\n",
        "    steps = np.arange(0, size, batch_size).tolist()\n",
        "    idx = 0\n",
        "\n",
        "    while idx < len(steps):\n",
        "        if steps[idx] == steps[-1]:\n",
        "            break\n",
        "        batch_df = data[steps[idx]:steps[idx+1]]\n",
        "        batches.append(batch_df)\n",
        "        idx += 1\n",
        "\n",
        "    print('Batch Size: ', batch_size,\n",
        "    '\\nBatches: ', num_batches,\n",
        "    '\\nOriginal: ', size)\n",
        "    return batches\n",
        "\n",
        "def count_token_frequency(article, data):\n",
        "    \"\"\"\n",
        "    Counts the frequency of words that appear in each senctence.\n",
        "    - designed for (chess, token_counts_df)\n",
        "    - Returns as a Pandas DataFrame.\n",
        "    \"\"\"\n",
        "    token_sent_freq = {}\n",
        "    for token in data.index:\n",
        "        counter = 0\n",
        "        for sent in article.sent_tokenize:\n",
        "            if token.lower() in sent.lower():\n",
        "                counter += 1\n",
        "        token_sent_freq[token] = [counter]\n",
        "    df = pd.DataFrame(token_sent_freq)\n",
        "    return df"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "argv": [
        "/home/kalen/anaconda3/bin/python",
        "-m",
        "ipykernel_launcher",
        "-f",
        "{connection_file}"
      ],
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}